{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Principal Component Analysis (PCA)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The main purposes of a principal component analysis are the analysis of data to identify patterns and finding patterns to reduce the dimensions of the dataset with minimal loss of information.\n",
    "\n",
    "Here, our desired outcome of the principal component analysis is to project a feature space (our dataset consisting of $n$ $d$-dimensional samples) onto a smaller subspace that represents our data \"well\". A possible application would be a pattern classification task, where we want to reduce the computational costs and the error of the parameter estimation by reducing the number of dimensions of our feature space by extracting a subspace that describes our data \"best\".\n",
    "\n",
    "### What is a \"good\" subspace?\n",
    "\n",
    "Let's assume that our goal is to reduce the dimensions of a $d$-dimensional dataset by projecting it onto a $k$-dimensional subspace (where $k < d$). So, how do we know what size we would choose for $k$, and how do we know if we have a feature space that represents our data \"well\"? Later, we will compute eigenvectors (the components) from our dataset and collect them in a so-called scatter-matrix (or alternatively calculate them from the covariance matrix). Each of those eigenvectors is associated with an eigenvalue, which tells us about the \"length\" or \"magnitude\" of the eigenvectors. If we observe that all the eigenvalues are of very similar magnitude, this is a good indicator that our data is already in a \"good\" subspace. Or if some of the eigenvalues are much higher than others, we might be interested in keeping only those eigenvectors with the much larger eigenvalues, since they contain more information about our data distribution. Vice versa, eigenvalues that are close to 0 are less informative and we might consider in dropping those when we construct the new feature subspace.\n",
    "\n",
    "### Summarizing the PCA approach\n",
    "\n",
    "Listed below are the 6 general steps for performing a principal component analysis, which we will investigate in the following sections.\n",
    "\n",
    "1. Take the whole dataset consisting of $d$-dimensional samples ignoring the class labels\n",
    "2. Compute the $d$-dimensional mean vector (i.e. the means for every dimension of the whole dataset)\n",
    "3. Compute the scatter matrix (alternatively, the covariance matrix) of the whole dataset\n",
    "4. Compute eigenvectors ($e_1,e_2,...,e_d$) and corresponding eigenvalues ($\\lambda_1,\\lambda_2,...,\\lambda_d$)\n",
    "5. Sort the eigenvectors by decreasing eigenvalues and choose $k$ eigenvectors with the largest eigenvalues to form a $d \\times k$ dimensional matrix $W$ (where every column represents an eigenvector)\n",
    "6. Use this $d \\times k$ eigenvector matrix to transform the samples onto the new subspace. This can be summarized by the mathematical equation: $y=W^T \\times x$ (where $x$ is a $d \\times 1$-dimensional vector representing one sample, and $y$ is the transformed $k \\times 1$-dimensional sample in the new subspace.)\n",
    "\n",
    "## Generating some 3-dimensional sample data\n",
    "\n",
    "For the following example, we will generate 40 3-dimensional samples randomly drawn from a multivariate Gaussian distribution.\n",
    "\n",
    "Here we will assume that the samples stem from two different classes, where one half (i.e. 20) samples of our dataset are labeled $\\omega_1$ (class 1) and the other half $\\omega_2$ (class 2).\n",
    "\n",
    "$\n",
    "\\mu_1 = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix} \\quad\n",
    "\\mu_2 =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{bmatrix} \\quad\n",
    "\\text{(sample means)}\n",
    "$\n",
    "\n",
    "$\n",
    "\\Sigma_1 =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix} \\quad\n",
    "\\Sigma_2 =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix} \\quad\n",
    "\\text{(covariance matrices)}\n",
    "$\n",
    "\n",
    "### Why are we choosing a 3-dimensional sample?\n",
    "\n",
    "The problem of multi-dimensional data is its visualization, which would make it quite tough to follow our example principal component analysis (at least visually). We could also choose a 2-dimensional sample dataset for the following examples, but since the goal of the PCA in a \"Dimensionality Reduction\" application is to drop at least one of the dimensions, I find it more intuitive and visually appealing to start with a 3-dimensional dataset that we reduce to a 2-dimensional dataset by dropping 1 dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mu_vec1 = np.zeros(3)\n",
    "cov_mat1 = np.eye(3)\n",
    "class1_sample = np.random.multivariate_normal(mu_vec1, cov_mat1, 20).T\n",
    "assert class1_sample.shape == (3, 20), \"The matrix has not the dimensions 3x20\"\n",
    "\n",
    "mu_vec2 = np.ones(3)\n",
    "cov_mat2 = np.eye(3)\n",
    "class2_sample = np.random.multivariate_normal(mu_vec2, cov_mat2, 20).T\n",
    "assert class2_sample.shape == (3, 20), \"The matrix has not the dimensions 3x20\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code above, we created two $3 \\times 20$ datasets - one dataset for each class $\\omega_1$ and $\\omega_2$ - where each column can be pictured as a 3-dimensional vector $x = \\begin{pmatrix}x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}$ so that our dataset will have the form\n",
    "\n",
    "$\n",
    "X = \n",
    "\\begin{pmatrix}\n",
    "x_{1_1} & x_{1_2} & ... & x_{1_{20}}\\\\\n",
    "x_{2_1} & x_{2_2} & ... & x_{2_{20}}\\\\\n",
    "x_{3_1} & x_{3_2} & ... & x_{3_{20}}\\\\\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "Just to get a rough idea how the samples of our two classes $\\omega_1$ and $\\omega_2$ are distributed, let us plot them in a 3D scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(class1_sample[0, :], class1_sample[1, :], class1_sample[2, :],\n",
    "        'o', markersize=8, color='blue', alpha=0.5, label='class1')\n",
    "ax.plot(class2_sample[0, :], class2_sample[1, :], class2_sample[2, :],\n",
    "        '^', markersize=8, color='red', alpha=0.5, label='class2')\n",
    "\n",
    "plt.title('Samples for class 1 and class 2')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Taking the whole dataset ignoring the class labels\n",
    "\n",
    "Because we don't need class labels for the PCA analysis, let us merge the samples for our 2 classes into on $3 \\times 40$-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = np.concatenate((class1_sample, class2_sample), axis=1)\n",
    "assert all_samples.shape == (3, 40), \"The matrix has not the dimensions 3x40\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computing the d-dimensional mean vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.42604619],\n",
       "       [0.17797879],\n",
       "       [0.42782438]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_vector = np.mean(all_samples, axis=1).reshape(-1, 1)\n",
    "mean_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. a) Computing the Scatter Matrix\n",
    "\n",
    "The scatter matrix is computed by the following equation:\n",
    "\n",
    "$S = \\sum_{k=1}^n (x_k-m)(x_k-m)^T$\n",
    "\n",
    "where $m$ is the mean vector\n",
    "\n",
    "$m = \\frac{1}{n}\\sum_{k=1}^n x_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33.64126129,  6.76796535,  6.50296559],\n",
       "       [ 6.76796535, 42.12451666, 10.07426467],\n",
       "       [ 6.50296559, 10.07426467, 35.33531371]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scatter_matrix = (all_samples - mean_vector).dot((all_samples - mean_vector).T)\n",
    "scatter_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. b) Computing the Covariance Matrix (alternatively to the scatter matrix)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
