{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Support Vector Machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression. Suppor Vector Machine is highly preferred by many as it produces significant accuracy with less computation power. In this section, we will develop the intuition behind support vector machines and their use in classification problems.\n",
    "\n",
    "## What is Support Vector Machine?\n",
    "\n",
    "The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space (N - the number of features) that distinctly classifies the data points.\n",
    "\n",
    "<img src='files/img/hyperplane.png'>\n",
    "\n",
    "To separate the two classes of data points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane that has the maximum margin, i.e. the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.\n",
    "\n",
    "## Hyperplanes and Support Vectors\n",
    "\n",
    "<img src='files/img/hyperplane3d.png'>\n",
    "\n",
    "Hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes. Also, the dimension of the hyperplane depends upon the number of features. If the number of input features is 2, then the hyperplane is just a line. If the number of input features is 3, then the hyperplane becomes a two-dimensional plane. It becomes difficult to imagine when the number of features exceed 3.\n",
    "\n",
    "<img src='files/img/supportvectors.jpg'>\n",
    "\n",
    "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### The primal form\n",
    "\n",
    "A support vector classifier can be represented mathematically by\n",
    "\n",
    "$$f(x) = w^T x - b$$\n",
    "\n",
    "using the output $f(x)$ to determine the class label vector $y$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    y =\n",
    "    \\begin{cases}\n",
    "        1, & f(x) \\geq 0 \\\\\n",
    "        -1, & f(x) < 0\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $x$ is a feature vector, $w$ is a weight vector, and $b$ is a bias term. For SVMs, the convention is to use class labels $y \\in \\{-1, 1\\}$. This function $f(x)$ represents the classifier's decision function. It looks similar to the problem posed by logistic regression, however with an SVM we don't want just any solution that will divide our data, we want the solution that will do so with the largest margin between our two classes (if possible). This introduces a constraint to our problem. What this ultimately means is that our optimization problem becomes\n",
    "\n",
    "$$min_{w, b} \\frac{||w||^2}{2}$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$y^{(i)}(w^T x^{(i)} - b) \\geq 1, i = 1,...,m$$\n",
    "\n",
    "where $m$ is the number of training examples, indexed by $i$. This formulation is known as the [primal formulation](https://en.wikipedia.org/wiki/Support_vector_machine#Primal_form) of the SVM optimization problem.\n",
    "\n",
    "This formulation can be solved using existing quadratic programming solvers such as [CVXOPT](http://cvxopt.org/) and other methods (for example see [this paper [PDF]](http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/neco_%5B0%5D.pdf). However, there is another formulation of the problem that can be solved without using quadratic programming techniques.\n",
    "\n",
    "### The dual form\n",
    "\n",
    "Using Lagrange duality, the optimization problem can be reformulated into the [dual form](https://en.wikipedia.org/wiki/Support_vector_machine#Dual_form) (see [lecture notes for Stanford's CS229 taught by Andrew Ng](http://cs229.stanford.edu/materials.html) for a walkthrough of the math if you're interested).\n",
    "\n",
    "$$max_{\\alpha}W(\\alpha) = \\sum_{i=1}^m \\alpha_i - \\frac{1}{2}\\sum_{i, j = 1}^m y^{(i)}y^{(j)}\\alpha_i \\alpha_j \\langle x^{(i)}, x^{(j)} \\rangle$$\n",
    "\n",
    "This problem is also subject to the following two constraints:\n",
    "\n",
    "$$0 \\leq \\alpha_i \\leq C, i = 1,...,m$$\n",
    "$$\\sum_{i=1}^m \\alpha_i y^{(i)} = 0$$\n",
    "\n",
    "Where,\n",
    "-  $m$ is the number of training examples\n",
    "-  $x^{(i)}$ is the $i^{th}$ training example feature vector\n",
    "-  $\\langle x^{(i)}, x^{(j)} \\rangle$ represents the inner product of the $x^{(i)}$ and $x^{(j)}$ feature vectors\n",
    "-  $y^{(i)}$ is the class for the $i^{th}$ training example\n",
    "-  $\\alpha_i$ is the Lagrange multiplier associated with the $i^{th}$ training example\n",
    "-  $C$ is a regularization parameter (larger values introduce less regularization)\n",
    "\n",
    "When using a kernel $K(x, z)$, with feature mapping function $\\phi(x)$, $\\langle x^{(i)}, x^{(j)} \\rangle$ is replaced with $\\langle \\phi(x^{(i)}), \\phi(x^{(j)})\\rangle$.\n",
    "\n",
    "When using the dual form, the SVM decision function is the following:\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^m \\alpha_i y^{(i)} \\langle x^{(i)}, x \\rangle + b$$\n",
    "\n",
    "Where $b$ is a scalar bias term we will calculate when training our model.\n",
    "\n",
    "## Approach\n",
    "\n",
    "For the sake of understanding the concepts behind support vector classification, we will instead implement a version of the [Sequential Minimal Optimization (SMO)](https://en.wikipedia.org/wiki/Sequential_minimal_optimization) algorithm as described by [John Platt in 1998 [PDF]](http://research.microsoft.com/pubs/69644/tr-98-14.pdf) to solve our optimization problem.\n",
    "\n",
    "SMO works by breaking down the dual form of the SVM optimization problem into many smaller optimization problems that are more easily solvable. In a nutshell, the algorithm works like this:\n",
    "\n",
    "-  Two multiplier values ($\\alpha_i$ and $\\alpha_j$) are selected out and their values are optimized while holding all other $\\alpha$ values constant.\n",
    "-  Once these two are optimized, another two are chosen and optimized over.\n",
    "-  Choosing and optimizing repeats until the convergence, which is determined based on the problem constraints. Heuristics are used to select the two $\\alpha$ values to optimize over, helping to speed up convergence. The heuristics are based on error cache that is stored while training the model.\n",
    "\n",
    "## What we're looking for\n",
    "\n",
    "What we want out of the algorithm is a vector of $\\alpha$ values that are mostly zeros, except where the corresponding training example is closest to the decision boundary. These examples are our support vectors and should lie near the decision boundary. We should end up with a few of them once our algorithm has converged. What this implies is that the resultant decision boundary will only depend on the training examples closest to it. If we were to add more examples to our training set that were far from the decision boundary, the support vectors would not change. However, labeled examples closer to the decision boundary can exert greater influence on the solution, subject to the degree of regularization. In other words, non-regularized (hard-margin) SVMs can be sensitive to outliers, while regularized (soft-margin) models are not.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "__Goal:__ Implement a two-class SVC that is able to make use of the kernel trick. Use SMO to solve the SVM optimization problem.\n",
    "\n",
    "To do so, we will use [numpy](http://www.numpy.org/) to handle our arrays, [matplotlib](http://matplotlib.org/) to visualize our data and [scikit-learn](http://scikit-learn.org/) to generate some toy data.\n",
    "\n",
    "__Note__: This notebook was written with Python 3.5, which has the `@` operator, an infix matrix multiplication operator. If you're using a version of Python that is earlier than 3.5, you will need to replace operations like `X @ Y` with `np.dot(X, Y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_circles, make_moons\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "To start off we will define a class to hold the information describing our model for convenient access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMOModel:\n",
    "    \"\"\"\n",
    "    Container object for the model used for sequential minimal optimization\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, C, kernel, alphas, b, errors):\n",
    "        self.X = X                # training data vector\n",
    "        self.y = y                # class label vector\n",
    "        self.C = C                # regularization parameter\n",
    "        self.kernel = kernel      # kernel function\n",
    "        self.alphas = alphas      # lagrange multiplier vector\n",
    "        self.b = b                # scalar bias term\n",
    "        self.errors = errors      # error cache\n",
    "        self._obj = []            # record of objective function value\n",
    "        self.m = len(self.X)      # store size of training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels\n",
    "\n",
    "Next, we will define two different kernel functions for our implementations. Depending on the kernel, different decision boundaries can be constructed. We will define a linear kernel and a Gaussian (also known as radial basis function or RBF) kernel.\n",
    "\n",
    "Our linear kernel will be defined as\n",
    "\n",
    "$$K(x, z) = x^T z + b$$\n",
    "\n",
    "where $x$ and $z$ are arrays of input feature vectors, and $b$ is an optional bias term we will set equal to one. This kernel calculates a pairwise linear combination of the points listed in $x$ and $z$. Using this kernel will results in the generation of a linear decision boundary.\n",
    "\n",
    "Our Gaussian kernel will be defined as\n",
    "\n",
    "$$K(x, z) = \\exp \\left(\\frac{-|x - z|^2}{2 \\sigma^2}\\right)$$\n",
    "\n",
    "where $x$ and $z$ are described as above and $\\sigma$ is a width parameter describing how wide the kernel is (this can be set based on the spacing between data points). This kernel calculates a Gaussian similarity between the training examples listed in $x$ and $z$, with a value of 1 indicating that the points have exactly the same feature vector and 0 indicating dissimilar vectors. Using this kernel allows for the construction of more complex, non-linear decision boundaries.\n",
    "\n",
    "Both of these functions should take two array of features and return a matrix of shape length $x$ by length $z$. SVMs can make use of many different kernels without any change in the code to train them, as long as the Gram matrix output by the kernel is positive, semi-definite (see these [lecture notes [PDF]](http://www.cs.cornell.edu/courses/cs4780/2011fa/lecture/08-svm_kernels.pdf) for more information).\n",
    "\n",
    "What this means for our code is that our kernels need to return matrices with a certain shape, specifically length $x$ by length $z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x, y, b=1):\n",
    "    \"\"\"\n",
    "    Returns the linear combination of arrays `x` and `y` with\n",
    "    the optional bias term `b` (set to 1 by default).\n",
    "    \"\"\"\n",
    "    return np.dot(x, y.T) + b\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=1):\n",
    "    \"\"\"\n",
    "    Returns the gaussian similarity of arrays `x` and `y` with\n",
    "    kernel width parameter `sigma` (set to 1 by default).\n",
    "    \"\"\"\n",
    "    if np.ndim(x) == 1 and np.ndim(y) == 1:\n",
    "        result = np.exp(-np.linalg.norm(x - y) ** 2 / (2 * sigma ** 2))\n",
    "    elif (np.ndim(x) > 1 and np.ndim(y) == 1) or (np.ndim(x) == 1 and np.ndim(y) > 1):\n",
    "        result = np.exp(-np.linalg.norm(x - y, axis=1) ** 2 / (2 * sigma ** 2))\n",
    "    elif np.ndim(x) > 1 and np.ndim(y) > 1:\n",
    "        result = np.exp(-np.linalg.norm(x[:, np.newaxis] - y[np.newaxis, :], axis=2) ** 2 / (2 * sigma ** 2))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that our kernels output matrices of the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len, y_len = 5, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_kernel(np.random.rand(x_len, 1), np.random.rand(y_len, 1)).shape == (x_len, y_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_kernel(np.random.rand(x_len, 1), np.random.rand(y_len, 1)).shape == (x_len, y_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective and decision function\n",
    "\n",
    "Up next are the dual form of the objective function and decision function as we described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function to optimize\n",
    "\n",
    "def objective_function(alphas, target, kernel, X_train):\n",
    "    \"\"\"\n",
    "    Returns the SVM objective function based in the input model\n",
    "    defined by:\n",
    "    \n",
    "    `alphas`: vector of Lagrange multipliers\n",
    "    `target`: vector of class labels (-1 or 1) for training data\n",
    "    `kernel`: kernel function\n",
    "    `X_train`: training data for model.\n",
    "    \"\"\"\n",
    "    return np.sum(alphas) - 0.5 * np.sum((target[:, None] * target[None, :]) *\n",
    "                                         (alphas[:, None] * alphas[None, :]) *\n",
    "                                         (kernel(X_train, X_train)))\n",
    "\n",
    "# Decision function\n",
    "\n",
    "def decision_function(alphas, target, kernel, X_train, x_test, b):\n",
    "    \"\"\"\n",
    "    Applies the SVM decision function to the input feature vectors\n",
    "    in `x_test`.\n",
    "    \"\"\"\n",
    "    result = np.dot((alphas * target), kernel(X_train, x_test)) - b\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into the algorithm, here is a function to plot the decision boundary of our model on a given axis. This will be useful later to see the results of our SMO implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, ax, resolution=100, colors=('b', 'k', 'r'), levels=(-1, 0, 1)):\n",
    "    \"\"\"\n",
    "    Plots the model's decision boundary on the input axes object.\n",
    "    Range of decision boundary grid is determined by the training data.\n",
    "    Returns decision boundary grid and axes object (`grid`, `ax`).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate coordinate grid of shape [resolution x resolution]\n",
    "    # and evaluate the model over the entire space\n",
    "    xrange = np.linspace(model.X[:, 0].min(), model.X[:, 0].max(), resolution)\n",
    "    yrange = np.linspace(model.X[:, 1].min(), model.X[:, 1].max(), resolution)\n",
    "    xx, yy = np.meshgrid(xrange, yrange)\n",
    "    \n",
    "    grid = decision_function(model.alphas, model.y,\n",
    "                             model.kernel, model.X,\n",
    "                             np.array([xx.ravel(), yy.ravel()]).T, model.b)\n",
    "    \n",
    "    # Plot decision contours using grid and\n",
    "    # make a scatter plot of training data\n",
    "    ax.contour(xrange, yrange, grid, levels=levels, linewidths=(1, 1, 1),\n",
    "               linestyles=('--', '-', '--'), colors=colors)\n",
    "    ax.scatter(model.X[:, 0], model.X[:, 1], c=model.y,\n",
    "               cmap=plt.cm.viridis, lw =0, alpha=0.25)\n",
    "    \n",
    "    # Plot support vectors (non-zero alphas)\n",
    "    # as circledd points (linewidth > 0)\n",
    "    mask = np.round(model.alphas, decimals=2) != 0.0\n",
    "    ax.scatter(model.X[mask, 0], model.X[mask, 1], c=model.y[mask],\n",
    "               cmap=plt.cm.viridis, lw=1, edgecolors='k')\n",
    "    \n",
    "    return grid, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Minimal Optimization\n",
    "\n",
    "Now we will dig into the SMO algorithm to train our model. I based the code here off of the pseudocode provided in Platt's paper linked to above. The implementation here is slightly different. I changed the training functions to pass around our model object to make the variable scoping clearer.\n",
    "\n",
    "The three functions used to train our model are `take_step()`, `examine_example()`, and `train()`. These are structured to work as follows:\n",
    "\n",
    "1. The `train()` function implements selection of the first $\\alpha$ to optimize via the first choice heuristic and passes this value to `examine_example()`.\n",
    "2. Then `examine_example()` implements the second choice heuristic to choose the second $\\alpha$ to optimize, and passes the index of both $\\alpha$ values to `take_step()`.\n",
    "3. Finally `take_step()` carries out the meat of the calculations and computes the two new $\\alpha$ values, a new threshold $b$, and updates the error cache.\n",
    "\n",
    "The `train()` function uses a while loop to iterate through the $\\alpha$ values in a few different ways until no more optimizations can be made, at which point it returns the optimized $\\alpha$ vector (embedded in an `SMOModel` object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
