{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Support Vector Machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression. Suppor Vector Machine is highly preferred by many as it produces significant accuracy with less computation power. In this section, we will develop the intuition behind support vector machines and their use in classification problems.\n",
    "\n",
    "## What is Support Vector Machine?\n",
    "\n",
    "The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space (N - the number of features) that distinctly classifies the data points.\n",
    "\n",
    "<img src='files/img/hyperplane.png'>\n",
    "\n",
    "To separate the two classes of data points, there are many possible hyperplanes that could be chosen. Our objective is to find a plane that has the maximum margin, i.e. the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.\n",
    "\n",
    "## Hyperplanes and Support Vectors\n",
    "\n",
    "<img src='files/img/hyperplane3d.png'>\n",
    "\n",
    "Hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes. Also, the dimension of the hyperplane depends upon the number of features. If the number of input features is 2, then the hyperplane is just a line. If the number of input features is 3, then the hyperplane becomes a two-dimensional plane. It becomes difficult to imagine when the number of features exceed 3.\n",
    "\n",
    "<img src='files/img/supportvectors.jpg'>\n",
    "\n",
    "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.\n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, visualization=True):\n",
    "        self.visualization = visualization\n",
    "        self.colors = {1: 'r', -1: 'b'}\n",
    "        if self.visualization:\n",
    "            self.fig = plt.figure()\n",
    "            self.ax = self.fig.add_subplot()\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #{||w||: (w, b)}\n",
    "        opt_dict = {}\n",
    "        transforms = [[1, 1], [-1, 1], [-1, -1], [1, -1]]\n",
    "        \n",
    "        self.max_feature_value = np.max(X)\n",
    "        self.min_feature_value = np.min(X)\n",
    "        \n",
    "        # With smaller learning rate our margins and db will be more precise\n",
    "        learning_rates = [self.max_feature_value * 0.1,\n",
    "                          self.max_feature_value * 0.001,\n",
    "                          self.max_feature_value * 0.0001]\n",
    "        \n",
    "        b_range_multiple = 5\n",
    "        b_step_multiple = 5\n",
    "        \n",
    "        latest_optimum = self.max_feature_value * 10\n",
    "        \n",
    "        \"\"\"\n",
    "        Objective is to satisfy yi*(w.x + b) >= 1 for all training dataset such that ||w|| is minimum.\n",
    "        For this we will start with random w, and try to satisfy it with making b bigger and bigger.\n",
    "        \"\"\"\n",
    "        # Making step size smaller and smaller to get precise value\n",
    "        for lrate in learning_rates:\n",
    "            w = np.array([latest_optimum, latest_optimum])\n",
    "            \n",
    "            optimized = False\n",
    "            while not optimized:\n",
    "                # b = [-maxvalue to maxvalue] we wanna maximize the b values\n",
    "                # so check for every b value\n",
    "                for b in np.arange(-1 * self.max_feature_value * b_range_multiple,\n",
    "                                   self.max_feature_value * b_range_multiple,\n",
    "                                   lrate * b_step_multiple):\n",
    "                    \n",
    "                    for transformation in transforms:\n",
    "                        w_t = w * transformation\n",
    "                        \n",
    "                        # Every data point should be correct\n",
    "                        correctly_classified = True\n",
    "                        for yi in "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
