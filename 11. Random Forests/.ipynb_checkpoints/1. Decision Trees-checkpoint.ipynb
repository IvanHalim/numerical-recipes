{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision trees are a powerful prediction method and extremely popular.\n",
    "\n",
    "They are popular because the final model is so easy to understand by practicioners and domain experts alike. The final decision tree can explain exactly why a specific prediction was made, making it very attractive for operational use.\n",
    "\n",
    "Decision trees also provide the foundation for more advanced ensemble methods such as bagging, random forests and gradient boosting.\n",
    "\n",
    "In this tutorial, you will discover how to implement the Classification And Regression Tree algorithm from scratch with Python.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "-  How to calculate and evaluate candidate split points in a data.\n",
    "-  How to arrange splits into a decision tree structure.\n",
    "-  How to apply the classification and regression tree algorithm to a real problem.\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "<img src='files/img/decisiontree.png'>\n",
    "\n",
    "## Descriptions\n",
    "\n",
    "This section provides a brief introduction to the Classification and Regression Tree algorithm and the Banknote dataset used in this tutorial.\n",
    "\n",
    "### Classification and Regression Trees\n",
    "\n",
    "Classification or Regression Trees or CART for short is an acronym introduced by Leo Breiman to refer to Decision Tree algorithms that can be used for classification or regression predictive modeling problems.\n",
    "\n",
    "We will focus on using CART for classification in this tutorial.\n",
    "\n",
    "The representation of the CART model is a binary tree. This is the same binary tree from algorithms and data structures, nothing too fancy (each node can have zero, one or two child nodes).\n",
    "\n",
    "A node represents a single input variable (X) and a split point on that variable, assuming the variable is numeric. The leaf nodes (also called terminal nodes) of the tree contain an output variable (y) which is used to make a prediction.\n",
    "\n",
    "Once created, a tree can be navigated with a new row of data following each branch with the splits until a final prediction is made.\n",
    "\n",
    "Creating a binary decision tree is actually a process of dividing up the input space. A greedy approach is used to divide the space, called Recursive Binary Splitting. This is a numerical procedure where all the values are lined up and different split points are tried and tested using a cost function.\n",
    "\n",
    "The split with the best cost (lowest cost because we minimize cost) is selected. All input variables and all possible split points are evaluated and chosen in a greedy manner based on the cost function.\n",
    "\n",
    "-  __Regression__: The cost function that is minimized to choose split points is the sum squared error across all training samples that fall within the rectangle.\n",
    "-  __Classification__: The Gini cost function is used which provides an indication of how pure the nodes are, where node purity refers to how mixed the training data assigned to each node is.\n",
    "\n",
    "Splitting continues until nodes contain a minimum number of training examples or a maximum tree depth is reached.\n",
    "\n",
    "### Banknote Dataset\n",
    "\n",
    "The banknote dataset involves predicting whether a given banknote is authentic given a number of measures taken from a photograph.\n",
    "\n",
    "The dataset contains 1,372 rows with 5 numeric variables. It is a classification problem with two classes (binary classification).\n",
    "\n",
    "Below provides a list of the five variables in the dataset.\n",
    "1. Variance of Wavelet Transformed image (continuous).\n",
    "2. Skewness of Wavelet Transformed image (continuous).\n",
    "3. Kurtosis of Wavelet Transformed image (continuous).\n",
    "4. Entropy of image (continuous).\n",
    "5. Class (integer)\n",
    "\n",
    "Below is a sample of the first 5 rows in the dataset\n",
    "```\n",
    " 3.6216,8.6661,-2.8073,-0.44699,0\n",
    " 4.5459,8.1674,-2.4586,-1.4621,0\n",
    " 3.866,-2.6383,1.9242,0.10645,0\n",
    " 3.4566,9.5228,-4.0112,-3.5944,0\n",
    " 0.32924,-4.4552,4.5718,-0.9888,0\n",
    "```\n",
    "\n",
    "Using the Zero Rule Algorithm to predict the most common class value, the baseline accuracy on the problem is about 50%.\n",
    "\n",
    "You can learn more and download the dataset from the [UCI Machine Learning Repository.](http://archive.ics.uci.edu/ml/datasets/banknote+authentication)\n",
    "\n",
    "Download the dataset and place it in your current working directory with the filename __data_banknote_authentication.csv__.\n",
    "\n",
    "## Tutorial\n",
    "\n",
    "This tutorial is broken down into 5 parts:\n",
    "1. Gini Index.\n",
    "2. Create Split.\n",
    "3. Build a Tree.\n",
    "4. Make a Prediction.\n",
    "5. Banknote Case Study.\n",
    "\n",
    "These steps will give you the foundation that you need to implement the CART algorithm from scratch and apply it to your own predictive modeling problems.\n",
    "\n",
    "### 1. Gini Index\n",
    "\n",
    "The Gini index is the name of the cost function used to evaluate splits in the dataset.\n",
    "\n",
    "A split in the dataset involves one input attribute and one value for that attribute. It can be used to divide training patterns into two groups of rows.\n",
    "\n",
    "A Gini score gives an idea of how good a split is by how mixed the classes are in the two groups created by the split. A perfect separation results in a Gini score of 0, whereas the worst case split that results in 50/50 classes in each group, results in a Gini score of 0.5 (for a 2 class problem).\n",
    "\n",
    "Calculating Gini is best demonstrated with an example.\n",
    "\n",
    "We have two groups of data with 2 rows in each group. The rows in the first group all belong to class 0 and the rows in the second group belong to class 1, so it's a perfect split.\n",
    "\n",
    "We first need to calculate the proportion of classes in each group.\n",
    "\n",
    "$$\\text{proportion} = \\frac{\\text{count(class value)}}{\\text{count(rows)}}$$\n",
    "\n",
    "The proportion for this example would be:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\text{Group 1 class 0} = 2 / 2 = 1\\\\\n",
    "\\text{Group 1 class 1} = 0 / 2 = 0\\\\\n",
    "\\text{Group 2 class 0} = 0 / 2 = 0\\\\\n",
    "\\text{Group 2 class 1} = 2 / 2 = 1\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Gini is then calculated for each child node as follows:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\text{Gini index} &= \\sum(\\text{proportion} * (1.0 - \\text{proportion}))\\\\\n",
    "&= 1.0 - \\sum(\\text{proportion}^2)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "The Gini index for each group must then be weighted by the size of the group, relative to all of the samples in the parent, e.g. all samples that are currently being grouped. We can add this weighting to the Gini calculation for a group as follows:\n",
    "\n",
    "$$\\text{Gini index} = (1.0 - \\sum(\\text{proportion}^2)) * \\frac{\\text{group size}}{\\text{total samples}}$$\n",
    "\n",
    "In this example, the Gini scores for each group are calculated as follows:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\text{Gini(group 1)} &= (1 - (1^2 + 0^2)) * 2/4\\\\\n",
    "&= 0 * 0.5\\\\\n",
    "&= 0\\\\\n",
    "\\text{Gini(group 2)} &= (1 - (0^2 + 1^2)) * 2/4\\\\\n",
    "&= 0 * 0.5\\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "The scores are then added across each child node at the split point to give a final Gini score for the split point that can be compared to other candidate split points.\n",
    "\n",
    "The Gini for this split point would then be calculated as 0.0 + 0.0 or a perfect Gini score of 0.0.\n",
    "\n",
    "Below is a function named __gini_index()__ that calculates the Gini index for a list of groups and a list of known class values.\n",
    "\n",
    "You can see that there are some safety checks in there to avoid a divide by zero for an empty group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "    \n",
    "    # Count total samples\n",
    "    total_samples = float(sum([len(group) for group in groups]))\n",
    "    \n",
    "    # Sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        \n",
    "        # Avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        \n",
    "        # Score the group based on the score for each class\n",
    "        score = 0.0\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p**2\n",
    "            \n",
    "        # Weight the groups score by its relative size\n",
    "        gini += (1.0 - score) * (size / total_samples)\n",
    "        \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this function with our worked example above. We can also test it for the worst case of a 50/50 split in each group. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "groups1 = [[[1, 1], [1, 0]], [[1, 1], [1, 0]]]\n",
    "groups2 = [[[1, 0], [1, 0]], [[1, 1], [1, 1]]]\n",
    "classes = [0, 1]\n",
    "\n",
    "print(gini_index(groups1, classes))\n",
    "print(gini_index(groups2, classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example prints the two Gini scores, first the score for the worst case at 0.5 followed by the score for the best case at 0.0.\n",
    "\n",
    "Now that we know how to evaluate the results of a split, let's look at creating splits.\n",
    "\n",
    "### 2. Create Split\n",
    "\n",
    "A split is comprised of an attribute in the dataset and a value.\n",
    "\n",
    "We can summarize this as the index of an attribute to split and the value by which to split rows on that attribute. This is just a useful shorthand for indexing into rows of data.\n",
    "\n",
    "Creating a split involves three parts, the first we have already looked at which is calculating the Gini score. The remaining two parts are:\n",
    "1. Splitting a dataset\n",
    "2. Evaluating all splits.\n",
    "\n",
    "Let's take a look at each.\n",
    "\n",
    "#### 2.1. Splitting a Dataset\n",
    "\n",
    "Splitting a dataset means separating a dataset into two lists of rows given the index of an attribute and a split value for that attribute.\n",
    "\n",
    "Once we have the two groups, we can then use our Gini score above to evaluate the cost of the split.\n",
    "\n",
    "Splitting a dataset involves iterating over each row, checking if the attribute value is below or above the split value and assigning it to the left or right group respectively.\n",
    "\n",
    "Below is a function named __test_split()__ that implements this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset based on an attribute and a split value\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = [], []\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much to it.\n",
    "\n",
    "Note that the right group contains all rows with a value at the index above or equal to the split value.\n",
    "\n",
    "#### 2.2. Evaluating All Splits\n",
    "\n",
    "With the Gini function above and the test split function, we now have everything we need to evaluate splits.\n",
    "\n",
    "Given a dataset, we must check every value on each attribute as a candidate split, evaluate the cost of the split and find the best possible split we could make.\n",
    "\n",
    "Once the best split is found, we can use it as a node in our decision tree.\n",
    "\n",
    "This is an exhaustive and greedy algorithm.\n",
    "\n",
    "We will use a dictionary to represent a node in the decision tree as we can store data by name. When selecting the best split and using it as a new node for the tree, we will store the index of the chosen attribute, the value of that attribute by which to split and the two groups of data split by the chosen split point.\n",
    "\n",
    "Each group of data is its own small dataset of just those rows assigned to the left or right group by the splitting process. You can imagine how we might split each group again, recursively as we build out our decision tree.\n",
    "\n",
    "Below is a function named __get_split()__ that implements this procedure. You can see that it iterates over each attribute (except the class value) and then each value for that attribute, splitting and evaluating splits as it goes.\n",
    "\n",
    "The best split is recorded and then returned after all checks are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(dataset):\n",
    "    # Get a list of all unique class values\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    \n",
    "    # Record best values\n",
    "    b_index, b_value, b_gini, b_groups = 999, 999, 999, None\n",
    "\n",
    "    # Iterate over all the elements in the dataset\n",
    "    for index in range(len(dataset[0])-1):\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini   = gini_index(groups, class_values)\n",
    "            \n",
    "            # If the split has a lower gini score\n",
    "            if gini < b_gini:\n",
    "                b_index  = index\n",
    "                b_value  = row[index]\n",
    "                b_gini   = gini\n",
    "                b_groups = groups\n",
    "    \n",
    "    return {'index': b_index, 'value': b_value, 'groups': b_groups}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can contrive a small dataset to test out this function and our whole dataset splitting process.\n",
    "\n",
    "X1\t\t    | X2          | Y\n",
    "----------- | ----------- | -\n",
    "2.771244718 | 1.784783929 | 0\n",
    "1.728571309\t| 1.169761413 | 0\n",
    "3.678319846 | 2.81281357  | 0\n",
    "3.961043357 | 2.61995032  | 0\n",
    "2.999208922 | 2.209014212 | 0\n",
    "7.497545867 | 3.162953546 | 1\n",
    "9.00220326  | 3.339047188 | 1\n",
    "7.444542326 | 0.476683375 | 1\n",
    "10.12493903 | 3.234550982 | 1\n",
    "6.642287351 | 3.319983761 | 1\n",
    "\n",
    "We can plot this dataset using separate colors for each class. You can see that it would not be difficult to manually pick a value of X1 (x-axis on the plot) to split this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = [[2.771244718, 1.784783929, 0],\n",
    "           [1.728571309, 1.169761413, 0],\n",
    "           [3.678319846, 2.81281357, 0],\n",
    "           [3.961043357, 2.61995032, 0],\n",
    "           [2.999208922, 2.209014212, 0],\n",
    "           [7.497545867, 3.162953546, 1],\n",
    "           [9.00220326, 3.339047188, 1],\n",
    "           [7.444542326, 0.476683375, 1],\n",
    "           [10.12493903, 3.234550982, 1],\n",
    "           [6.642287351, 3.319983761, 1]]\n",
    "\n",
    "X1 = [row[0] for row in dataset]\n",
    "X2 = [row[1] for row in dataset]\n",
    "Y = [row[-1] for row in dataset]\n",
    "\n",
    "plt.scatter(X1, X2, c=Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below puts all of this together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 < 2.771\tGini=0.444\n",
      "X1 < 1.729\tGini=0.5\n",
      "X1 < 3.678\tGini=0.286\n",
      "X1 < 3.961\tGini=0.167\n",
      "X1 < 2.999\tGini=0.375\n",
      "X1 < 7.498\tGini=0.286\n",
      "X1 < 9.002\tGini=0.375\n",
      "X1 < 7.445\tGini=0.167\n",
      "X1 < 10.125\tGini=0.444\n",
      "X1 < 6.642\tGini=0.0\n",
      "X2 < 1.785\tGini=0.5\n",
      "X2 < 1.17\tGini=0.444\n",
      "X2 < 2.813\tGini=0.32\n",
      "X2 < 2.62\tGini=0.417\n",
      "X2 < 2.209\tGini=0.476\n",
      "X2 < 3.163\tGini=0.167\n",
      "X2 < 3.339\tGini=0.444\n",
      "X2 < 0.477\tGini=0.5\n",
      "X2 < 3.235\tGini=0.286\n",
      "X2 < 3.32\tGini=0.375\n",
      "Split: X1 < 6.642\n"
     ]
    }
   ],
   "source": [
    "# Split a dataset based on an attribute and a split value\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = [], []\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "    \n",
    "    # Count total samples\n",
    "    total_samples = float(sum([len(group) for group in groups]))\n",
    "    \n",
    "    # Sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        \n",
    "        # Avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        \n",
    "        # Score the group based on the score for each class\n",
    "        score = 0.0\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p**2\n",
    "            \n",
    "        # Weight the groups score by its relative size\n",
    "        gini += (1.0 - score) * (size / total_samples)\n",
    "        \n",
    "    return gini\n",
    "\n",
    "def get_split(dataset):\n",
    "    # Get a list of all unique class values\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    \n",
    "    # Record best values\n",
    "    b_index, b_value, b_gini, b_groups = 999, 999, 999, None\n",
    "\n",
    "    # Iterate over all the elements in the dataset\n",
    "    for index in range(len(dataset[0])-1):\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini   = gini_index(groups, class_values)\n",
    "            print('X{0} < {1}\\tGini={2}'.format(index + 1, round(row[index], 3), round(gini, 3)))\n",
    "            \n",
    "            # If the split has a lower gini score\n",
    "            if gini < b_gini:\n",
    "                b_index  = index\n",
    "                b_value  = row[index]\n",
    "                b_gini   = gini\n",
    "                b_groups = groups\n",
    "    \n",
    "    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
    "\n",
    "dataset = [[2.771244718, 1.784783929, 0],\n",
    "           [1.728571309, 1.169761413, 0],\n",
    "           [3.678319846, 2.81281357, 0],\n",
    "           [3.961043357, 2.61995032, 0],\n",
    "           [2.999208922, 2.209014212, 0],\n",
    "           [7.497545867, 3.162953546, 1],\n",
    "           [9.00220326, 3.339047188, 1],\n",
    "           [7.444542326, 0.476683375, 1],\n",
    "           [10.12493903, 3.234550982, 1],\n",
    "           [6.642287351, 3.319983761, 1]]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    split = get_split(dataset)\n",
    "    print('Split: X{0} < {1}'.format(split['index'] + 1, round(split['value'], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to find the best split points in a dataset or list of rows, let's see how we can use it to build our decision tree.\n",
    "\n",
    "### 3. Build a Tree\n",
    "\n",
    "Creating the root node of the tree is easy. We call the above __get_split()__ function using the entire dataset.\n",
    "\n",
    "Adding more nodes to our tree is more interesting.\n",
    "\n",
    "Building a tree may be divided into 3 main parts:\n",
    "1. Terminal nodes\n",
    "2. Recursive splitting\n",
    "3. Building a tree\n",
    "\n",
    "#### 3.1. Terminal Nodes\n",
    "\n",
    "We need to decide when to stop growing a tree.\n",
    "\n",
    "We can do that using the depth and the number of rows that the node is responsible for in the training dataset.\n",
    "\n",
    "-  __Maximum Tree Depth__<br>\n",
    "   This is the maximum number of nodes from the root node of the tree. Once a maximum depth of the tree is met, we must stop splitting adding new nodes. Deeper trees are more complex and are more likely to overfit the training data.\n",
    "-  __Minimum Node Records__<br>\n",
    "   This is the minimum number of training patterns that a given node is responsible for. Once at or below this minimum, we must stop splitting adding new nodes. Nodes that account for too few training patterns are expected to be too specific and are likely to overfit the training data.\n",
    "   \n",
    "These two approaches will be user-specified arguments to our tree building procedure.\n",
    "\n",
    "There is one more condition. It is possible to choose a split in which all rows belong to one group. In this case, we will be unable to continue splitting and adding child nodes as we will have no records to split on one side or another.\n",
    "\n",
    "Now we have some ideas of when to stop growing the tree. When we do stop growing at a given point, that node is called a terminal node and is used to make a final prediction.\n",
    "\n",
    "This is done by taking the group of rows assigned to that node and selecting the most common class value in the group. This will be used to make predictions.\n",
    "\n",
    "Below is a function named __to_terminal()__ that will select a class value for a group of rows. It returns the most common output value in a list of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    # _, max_occur = max((outcomes.count(x), x) for x in set(outcomes))\n",
    "    return max(set(outcomes), key=outcomes.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Recursive Splitting\n",
    "\n",
    "We know how and when to create terminal nodes, now we can build our tree.\n",
    "\n",
    "Building a decision tree involves calling the above developed __get_split()__ function over and over again on the groups created for each node.\n",
    "\n",
    "New nodes added to an existing node are called child nodes. A node may have zero children (a terminal node), one child (one side makes a prediction directly) or two child nodes. We will refer to the child nodes as left and right in the dictionary representation of a given node.\n",
    "\n",
    "Once a node is created, we can create child nodes recursively on each group of data from the split by calling the same function again.\n",
    "\n",
    "Below is a function that implements this recursive procedure. It takes a node as an argument as well as the maximum depth, minimum number of patterns in a node and the current depth of a node.\n",
    "\n",
    "You can imagine how this might be first called passing in the root node and the depth of 1. This function is best explained in steps:\n",
    "1. Firstly, the two groups of data split by the node are extracted for use and deleted from the node. As we work on these groups, the node no longer requires access to these data.\n",
    "2. Next, we check if either left or right group of rows is empty and if so, we create a terminal node using what records we do have.\n",
    "3. We then check if we have reached our maximum depth and if so we create a terminal node.\n",
    "4. We then process the left child, creating a terminal node if the group of rows is too small, otherwise creating and adding the left node in a depth first fashion until the bottom of the tree is reached on this branch.\n",
    "5. The right side is then processed in the same manner, as we rise back up the constructed tree to the root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    \n",
    "    # Check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    \n",
    "    # Check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    \n",
    "    # Process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left)\n",
    "        split(node['left'], max_depth, min_size, depth + 1)\n",
    "        \n",
    "    # Process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right)\n",
    "        split(node['right'], max_depth, min_size, depth + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Building a Tree\n",
    "\n",
    "We can now put all of the pieces together.\n",
    "\n",
    "Building the tree involves creating the root node and calling the __split()__ function that then calls itself recursively to build out the whole tree.\n",
    "\n",
    "Below is the small __build_tree()__ function that implements this procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size):\n",
    "    root = get_split(train)\n",
    "    split(root, max_depth, min_size, 1)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test out this whole procedure using the small dataset we contrived above.\n",
    "\n",
    "Below is the complete example.\n",
    "\n",
    "Also included is a small __print_tree()__ function that recursively prints out nodes of the decision tree with one line per node. Although not as striking as a real decision tree diagram, it gives an idea of the tree structure and decisions made throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X1 < 6.642287351]\n",
      " [0]\n",
      " [1]\n"
     ]
    }
   ],
   "source": [
    "# Split a dataset based on an attribute and a split value\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = [], []\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "    \n",
    "    # Count total samples\n",
    "    total_samples = float(sum([len(group) for group in groups]))\n",
    "    \n",
    "    # Sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        \n",
    "        # Avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        \n",
    "        # Score the group based on the score for each class\n",
    "        score = 0.0\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p**2\n",
    "            \n",
    "        # Weight the groups score by its relative size\n",
    "        gini += (1.0 - score) * (size / total_samples)\n",
    "        \n",
    "    return gini\n",
    "\n",
    "def get_split(dataset):\n",
    "    # Get a list of all unique class values\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    \n",
    "    # Record best values\n",
    "    b_index, b_value, b_gini, b_groups = 999, 999, 999, None\n",
    "\n",
    "    # Iterate over all the elements in the dataset\n",
    "    for index in range(len(dataset[0])-1):\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini   = gini_index(groups, class_values)\n",
    "            \n",
    "            # If the split has a lower gini score\n",
    "            if gini < b_gini:\n",
    "                b_index  = index\n",
    "                b_value  = row[index]\n",
    "                b_gini   = gini\n",
    "                b_groups = groups\n",
    "    \n",
    "    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
    "\n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    \n",
    "    # Check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    \n",
    "    # Check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    \n",
    "    # Process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left)\n",
    "        split(node['left'], max_depth, min_size, depth + 1)\n",
    "        \n",
    "    # Process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right)\n",
    "        split(node['right'], max_depth, min_size, depth + 1)\n",
    "        \n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size):\n",
    "    root = get_split(train)\n",
    "    split(root, max_depth, min_size, 1)\n",
    "    return root\n",
    "\n",
    "def print_tree(node, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print('{0}[X{1} < {2}]'.format(depth * ' ', node['index'] + 1, node['value']))\n",
    "        print_tree(node['left'], depth + 1)\n",
    "        print_tree(node['right'], depth + 1)\n",
    "    else:\n",
    "        print('{0}[{1}]'.format(depth * ' ', node))\n",
    "\n",
    "dataset = [[2.771244718, 1.784783929, 0],\n",
    "           [1.728571309, 1.169761413, 0],\n",
    "           [3.678319846, 2.81281357, 0],\n",
    "           [3.961043357, 2.61995032, 0],\n",
    "           [2.999208922, 2.209014212, 0],\n",
    "           [7.497545867, 3.162953546, 1],\n",
    "           [9.00220326, 3.339047188, 1],\n",
    "           [7.444542326, 0.476683375, 1],\n",
    "           [10.12493903, 3.234550982, 1],\n",
    "           [6.642287351, 3.319983761, 1]]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tree = build_tree(dataset, 1, 1)\n",
    "    print_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a maximum depth of 1 (the second parameter in the call to the __build_tree()__ function), we can see that the tree uses the perfect split we discovered in the previous section. This is a tree with one node, also called a decision stump.\n",
    "\n",
    "We can vary the maximum depth argument as we run this example and see the effect on the printed tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X1 < 6.642287351]\n",
      " [X1 < 2.771244718]\n",
      "  [0]\n",
      "  [0]\n",
      " [X1 < 7.497545867]\n",
      "  [1]\n",
      "  [1]\n"
     ]
    }
   ],
   "source": [
    "tree = build_tree(dataset, 2, 1)\n",
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the maximum depth to 2, we are forcing the tree to make splits even when none are required. The __X1__ attribute is then used again by both the left and right children of the root node to split up the already perfect mix of classes.\n",
    "\n",
    "Finally, and perversely, we can force one more level of splits with a maximum depth of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X1 < 6.642287351]\n",
      " [X1 < 2.771244718]\n",
      "  [0]\n",
      "  [X1 < 2.771244718]\n",
      "   [0]\n",
      "   [0]\n",
      " [X1 < 7.497545867]\n",
      "  [X1 < 7.444542326]\n",
      "   [1]\n",
      "   [1]\n",
      "  [X1 < 7.497545867]\n",
      "   [1]\n",
      "   [1]\n"
     ]
    }
   ],
   "source": [
    "tree = build_tree(dataset, 3, 1)\n",
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tests show that there is great opportunity to refine the implementation to avoid unnecessary splits. This is left as an extension.\n",
    "\n",
    "Now that we can create a decision tree, let's see how we can use it to make predictions on new data.\n",
    "\n",
    "### 4. Make a Prediction\n",
    "\n",
    "Making predictions with a decision tree involves navigating the tree with the specifically provided row of data.\n",
    "\n",
    "Again we can implement this using a recursive function, where the same prediction routine is called again with the left or the right child nodes, depending on how the split affects the provided data.\n",
    "\n",
    "We must check if a child node is either a terminal value to be returned as the prediction, or if it is a dictionary node containing another level of the tree to be considered.\n",
    "\n",
    "Below is the __predict()__ function that implements this procedure. You can see how the index and value in a given node is used to evaluate whether the row of provided data falls on the left or the right of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    if isinstance(node, dict):\n",
    "        if row[node['index']] < node['value']:\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return predict(node['right'], row)\n",
    "    else:\n",
    "        return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our contrived dataset to test this function. The example makes a prediction for each row in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 0, Got: 0\n",
      "Expected: 0, Got: 0\n",
      "Expected: 0, Got: 0\n",
      "Expected: 0, Got: 0\n",
      "Expected: 0, Got: 0\n",
      "Expected: 1, Got: 1\n",
      "Expected: 1, Got: 1\n",
      "Expected: 1, Got: 1\n",
      "Expected: 1, Got: 1\n",
      "Expected: 1, Got: 1\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    if isinstance(node, dict):\n",
    "        if row[node['index']] < node['value']:\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return predict(node['right'], row)\n",
    "    else:\n",
    "        return node\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    dataset = [[2.771244718, 1.784783929, 0],\n",
    "               [1.728571309, 1.169761413, 0],\n",
    "               [3.678319846, 2.81281357, 0],\n",
    "               [3.961043357, 2.61995032, 0],\n",
    "               [2.999208922, 2.209014212, 0],\n",
    "               [7.497545867, 3.162953546, 1],\n",
    "               [9.00220326, 3.339047188, 1],\n",
    "               [7.444542326, 0.476683375, 1],\n",
    "               [10.12493903, 3.234550982, 1],\n",
    "               [6.642287351, 3.319983761, 1]]\n",
    "\n",
    "    # Predict with a stump\n",
    "    tree = build_tree(dataset, 1, 1)\n",
    "    for row in dataset:\n",
    "        prediction = predict(tree, row)\n",
    "        print('Expected: {0}, Got: {1}'.format(row[-1], prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know how to create a decision tree and use it to make predictions. Now let's apply it to a real dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
