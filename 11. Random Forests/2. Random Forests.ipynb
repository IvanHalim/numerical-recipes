{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Decision Trees can suffer from high variance which makes their results fragile to the specific training data used.\n",
    "\n",
    "Building multiple models from samples of your training data, called bagging, can reduce this variance, but the trees are highly correlated.\n",
    "\n",
    "Random Forest is an extension of bagging that in addition to building trees based on multiple samples of your training data, it also constrains the features that can be used to build the trees, forcing trees to be different. This, in turn, can give a lift in performance.\n",
    "\n",
    "In this tutorial, you will discover how to implement the Random Forest algorithm from scratch in Python.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "-  The difference between bagged decision trees and the random forest algorithm.\n",
    "-  How to construct bagged decision trees with more variance.\n",
    "-  How to apply the random forest algorithm to a predictive modelinng problem.\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "<img src='files/img/randomforest.png'>\n",
    "\n",
    "## Random Forest Algorithm\n",
    "\n",
    "Decision trees involve the greedy selection of the best split point from the dataset at each step.\n",
    "\n",
    "This algorithm makes decision trees susceptible to high variance if they are not pruned. This high variance can be harnessed and reduced by creating multiple trees with different samples of the training dataset (different views of the problem) and combining their predictions. This approach is called bootstrap aggregation or bagging for short.\n",
    "\n",
    "A limitation of bagging is that the same greedy algorithm is used to create each tree, meaning that it is likely that the same or very similar split points will be chosen in each tree making the different trees very similar (trees will be correlated). This, in turn, makes their predictions similar, mitigating the variance originally sought.\n",
    "\n",
    "We can force the decision trees to be different by limiting the features (rows) that the greedy algorithm can evaluate at each split point when creating the tree. This is called the Random Forest algorithm.\n",
    "\n",
    "Like bagging, multiple samples of the training dataset are taken and a different tree trained on each. The difference is that at each point a split is made in the data and added to the tree, only a fixed subset of attributes can be considered.\n",
    "\n",
    "For classification problems, the type of problems we will look at in this tutorial, the number of attributes to be considered for the split is limited to the square root of the number of input features.\n",
    "\n",
    "$$\\text{number of features for split} = \\sqrt{\\text{total input features}}$$\n",
    "\n",
    "The result of this one small change are trees that are more different from each other (uncorrelated) resulting in predictions that are more diverse and a combined prediction that often has better performance than a single tree or bagging alone.\n",
    "\n",
    "## Sonar Dataset\n",
    "\n",
    "The dataset we will use in this tutorial is the Sonar dataset.\n",
    "\n",
    "This is a dataset that describes sonar chirp returns bouncing off different surfaces. The 60 input variables are the strength of the returns at different angles. It is a binary classification problem that requires a model to differentiate rocks from metal cylinders. There are 208 observations.\n",
    "\n",
    "It is a well-understood dataset. All of the variables are continuous and generally in the range of 0 to 1. The output variable is a string \"M\" for mine and \"R\" for rock, which will need to be converted to integers 1 and 0.\n",
    "\n",
    "By predicting the class with the most observations in the dataset (M or mines) the Zero Rule Algorithm can achieve an accuracy of 53%.\n",
    "\n",
    "You can learn more about this dataset at the [UCI Machine Learning repository.](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks))\n",
    "\n",
    "Download the dataset for free and place it in your working directory with the filename __sonar.all-data.csv__.\n",
    "\n",
    "## Tutorial\n",
    "\n",
    "This tutorial is broken down into 2 steps.\n",
    "\n",
    "1. Calculating Splits.\n",
    "2. Sonar Dataset Case Study.\n",
    "\n",
    "These steps provide the foundation that you need to implement and apply the Random Forest algorithm to your own predictive modeling problems.\n",
    "\n",
    "### 1. Calculating Splits\n",
    "\n",
    "In a decision tree, split points are chosen by finding the attribute and the value of that attribute that results in the lowest cost.\n",
    "\n",
    "For classification problems, this cost function is often the Gini index, that calculates the purity of the groups of data created by the split point. A Gini index of 0 is perfect purity where class values are perfectly separated into two groups, in the case of a two-class classification problem.\n",
    "\n",
    "Finding the best split point in a decision tree involves evaluating the cost of each value in the training dataset for each input variable.\n",
    "\n",
    "For bagging and random forest, this procedure is executed upon a sample of the training dataset, made with replacement. Sampling with replacement means that the same row may be chosen and added to the sample more than once.\n",
    "\n",
    "We can update this procedure for Random Forest. Instead of enumerating all values for input attributes in search of the split with the lowest cost, we can create a sample of the input attributes to consider.\n",
    "\n",
    "This sample of input attributes can be chosen randomly and without replacement, meaning that each input attribute needs only be considered once when looking for the split point with the lowest cost.\n",
    "\n",
    "Below is a function named __get_split()__ that implements this procedure. It takes a dataset and a fixed number of input features from which to evaluate as input arguments, where the dataset may be a sample of the actual training dataset.\n",
    "\n",
    "The helper function __test_split()__ is used to split the dataset by a candidate split point and __gini_index()__ is used to evaluate the cost of a given split by the groups of rows created.\n",
    "\n",
    "We can see that a list of features is created by randomly selecting feature indices and adding them to a list (called __features__), this list of features is then enumerated and specific values in the training dataset are evaluated as split points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best split point for a dataset\n",
    "def get_split(dataset, n_features):\n",
    "    # Get a list of all unique class values\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    \n",
    "    # Record best values\n",
    "    b_index, b_value, b_gini, b_groups = 999, 999, 999, None\n",
    "    \n",
    "    # Randomly select n feature indices without replacement\n",
    "    features = random.sample(range(len(dataset[0])-1), n_features)\n",
    "\n",
    "    # Iterate over all the elements in the selected feature columns\n",
    "    for index in features:\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini   = gini_index(groups, class_values)\n",
    "            \n",
    "            # If the split has a lower gini score\n",
    "            if gini < b_gini:\n",
    "                b_index  = index\n",
    "                b_value  = row[index]\n",
    "                b_gini   = gini\n",
    "                b_groups = groups\n",
    "    \n",
    "    return {'index': b_index, 'value': b_value, 'groups': b_groups}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how a decision tree algorithm can be modified for use with the Random Forest algorithm, we can piece this together with an implementation of bagging and apply it to a real-world dataset.\n",
    "\n",
    "### 2. Sonar Dataset Case Study\n",
    "\n",
    "In this section, we will apply the Random Forest algorithm to the Sonar dataset.\n",
    "\n",
    "The example assumes that a CSV copy of the dataset is in the current working directory with the file name __sonar.all-data.csv__.\n",
    "\n",
    "The dataset is first loaded, the string values converted to numeric and the output column is converted from strings to the integer values of 0 and 1. This is achieved with helper functions __load_csv()__.\n",
    "\n",
    "We will use k-fold cross validation to estimate the performance of the learned model on unseen data. This means that we will construct and evaluate k models and estimate the performance as the mean model error. Classification accuracy will be used to evaluate each model. These behaviors are provided in the __cross_validation_split()__, __accuracy_metric()__ and __evaluate_algorithm()__ helper functions.\n",
    "\n",
    "We will also use an implementation of the Classification and Regression Trees (CART) algorithm adapted for bagging including the helper functions __test_split()__ to split a dataset into groups, __gini_index()__ to evaluate a split point, our modified __get_split()__ function discussed in the previous step, __to_terminal()__, __split()__ and __build_tree()__ used to create a single decision tree, __predict()__ to make a prediction with a decision tree, __subsample()__ to make a subsample of the training dataset and __bagging_predict()__ to make a prediction with a list of decision trees.\n",
    "\n",
    "A new function named __random_forest()__ is developed that first creates a list of decision trees from subsamples of the training dataset and then uses them to make predictions.\n",
    "\n",
    "As we stated above, the key difference between Random Forest and bagged decision trees is the one small change to the way that trees are created, here in the __get_split()__ function.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trees: 1\n",
      "Scores: [68.29268292682927, 73.17073170731707, 65.85365853658537, 68.29268292682927, 56.09756097560976]\n",
      "Mean Accuracy: 66.34146341463415%\n",
      "\n",
      "Trees: 5\n",
      "Scores: [65.85365853658537, 78.04878048780488, 78.04878048780488, 80.48780487804879, 75.60975609756098]\n",
      "Mean Accuracy: 75.60975609756098%\n",
      "\n",
      "Trees: 10\n",
      "Scores: [70.73170731707317, 87.8048780487805, 75.60975609756098, 87.8048780487805, 75.60975609756098]\n",
      "Mean Accuracy: 79.51219512195124%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Algorithm on Sonar Dataset\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    with open(filename) as f:\n",
    "        dataset = [[x for x in line.split(',')] for line in f if line.strip()]\n",
    "    return dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "    for row in dataset:\n",
    "        row[column] = float(row[column].strip())\n",
    "        \n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = set([row[column] for row in dataset])\n",
    "    lookup = {}\n",
    "    for i, value in enumerate(class_values):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "# Split a dataset into k-folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = []\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            index = random.randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = []\n",
    "    for fold in folds:\n",
    "        train_set = sum([f for f in folds if f != fold], [])\n",
    "        test_set = [row[:-1] + [None] for row in fold]\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "# Split a dataset based on an attribute and a split value\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = [], []\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, classes):\n",
    "    \n",
    "    # Count total samples\n",
    "    total_samples = float(sum([len(group) for group in groups]))\n",
    "    \n",
    "    # Sum weighted Gini index for each group\n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        \n",
    "        # Avoid divide by zero\n",
    "        if size == 0:\n",
    "            continue\n",
    "        \n",
    "        # Score the group based on the score for each class\n",
    "        score = 0.0\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p**2\n",
    "            \n",
    "        # Weight the groups score by its relative size\n",
    "        gini += (1.0 - score) * (size / total_samples)\n",
    "        \n",
    "    return gini\n",
    "\n",
    "# Select the best split point for a dataset\n",
    "def get_split(dataset, n_features):\n",
    "    # Get a list of all unique class values\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    \n",
    "    # Record best values\n",
    "    b_index, b_value, b_gini, b_groups = 999, 999, 999, None\n",
    "    \n",
    "    # Randomly select n feature indices without replacement\n",
    "    features = random.sample(range(len(dataset[0])-1), n_features)\n",
    "\n",
    "    # Iterate over all the elements in the selected feature columns\n",
    "    for index in features:\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)\n",
    "            gini   = gini_index(groups, class_values)\n",
    "            \n",
    "            # If the split has a lower gini score\n",
    "            if gini < b_gini:\n",
    "                b_index  = index\n",
    "                b_value  = row[index]\n",
    "                b_gini   = gini\n",
    "                b_groups = groups\n",
    "    \n",
    "    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
    "\n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    \n",
    "    # Check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    \n",
    "    # Check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    \n",
    "    # Process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left, n_features)\n",
    "        split(node['left'], max_depth, min_size, n_features, depth + 1)\n",
    "        \n",
    "    # Process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right, n_features)\n",
    "        split(node['right'], max_depth, min_size, n_features, depth + 1)\n",
    "        \n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    root = get_split(train, n_features)\n",
    "    split(root, max_depth, min_size, n_features, 1)\n",
    "    return root\n",
    "\n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):\n",
    "    if isinstance(node, dict):\n",
    "        if row[node['index']] < node['value']:\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return predict(node['right'], row)\n",
    "    else:\n",
    "        return node\n",
    "    \n",
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio):\n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    sample = random.choices(dataset, k=n_sample)\n",
    "    return sample\n",
    "\n",
    "# Make a prediction with a list of bagged trees\n",
    "def bagging_predict(trees, row):\n",
    "    predictions = [predict(tree, row) for tree in trees]\n",
    "    return max(set(predictions), key=predictions.count)\n",
    "\n",
    "# Random Forest Algorithm\n",
    "def random_forest(train, test, max_depth, min_size, sample_ratio, n_trees, n_features):\n",
    "    samples = [subsample(train, sample_ratio) for _ in range(n_trees)]\n",
    "    trees = [build_tree(sample, max_depth, min_size, n_features) for sample in samples]\n",
    "    predictions = [bagging_predict(trees, row) for row in test]\n",
    "    return predictions\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load data\n",
    "    dataset = load_csv('data/sonar.all-data.csv')\n",
    "    for i in range(len(dataset[0])-1):\n",
    "        str_column_to_float(dataset, i)\n",
    "    str_column_to_int(dataset, -1)\n",
    "    \n",
    "    # Evaluate algorithm\n",
    "    n_folds = 5\n",
    "    max_depth = 10\n",
    "    min_size = 1\n",
    "    sample_ratio = 1.0\n",
    "    n_features = int(math.sqrt(len(dataset[0])-1))\n",
    "    \n",
    "    for n_trees in [1, 5, 10]:\n",
    "        scores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_ratio, n_trees, n_features)\n",
    "        print('Trees: {0}'.format(n_trees))\n",
    "        print('Scores: {0}'.format(scores))\n",
    "        print('Mean Accuracy: {0}%\\n'.format(sum(scores) / float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _k_ value of 5 was used for cross-validation, giving each fold 208/5 = 41.6 or just over 40 records to be evaluated upon each iteration.\n",
    "\n",
    "Deep trees were constructed with a max depth of 10 and a minimum number of training rows at each node of 1. Samples of the training dataset were created with the same size as the original dataset, which is a default expectation for the Random Forest algorithm.\n",
    "\n",
    "The number of features considered at each split point was set to sqrt(num_features) or sqrt{60} = 7.74 rounded to 7 features.\n",
    "\n",
    "A suite of 3 different numbers of trees were evaluated for comparison, showing the increasing skill as more trees are added.\n",
    "\n",
    "## Extensions\n",
    "\n",
    "This section lists extensions to this tutorial that you my be interested in exploring.\n",
    "\n",
    "-  __Algorithm Tuning.__ The configuration used in the tutorial was found with a little trial and error but was not optimized. Experiment with larger number of trees, different numbers of features and even different tree configurations to improve performance.\n",
    "-  __More Problems.__ Apply the technique to other classification problems and even adapt it for regression with a new cost function and a new method for combining the predictions from trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
