{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "Decision Trees can suffer from high variance which makes their results fragile to the specific training data used.\n",
    "\n",
    "Building multiple models from samples of your training data, called bagging, can reduce this variance, but the trees are highly correlated.\n",
    "\n",
    "Random Forest is an extension of bagging that in addition to building trees based on multiple samples of your training data, it also constrains the features that can be used to build the trees, forcing trees to be different. This, in turn, can give a lift in performance.\n",
    "\n",
    "In this tutorial, you will discover how to implement the Random Forest algorithm from scratch in Python.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "-  The difference between bagged decision trees and the random forest algorithm.\n",
    "-  How to construct bagged decision trees with more variance.\n",
    "-  How to apply the random forest algorithm to a predictive modelinng problem.\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "## Random Forest Algorithm\n",
    "\n",
    "Decision trees involve the greedy selection of the best split point from the dataset at each step.\n",
    "\n",
    "This algorithm makes decision trees susceptible to high variance if they are not pruned. This high variance can be harnessed and reduced by creating multiple trees with different samples of the training dataset (different views of the problem) and combining their predictions. This approach is called bootstrap aggregation or bagging for short.\n",
    "\n",
    "A limitation of bagging is that the same greedy algorithm is used to create each tree, meaning that it is likely that the same or very similar split points will be chosen in each tree making the different trees very similar (trees will be correlated). This, in turn, makes their predictions similar, mitigating the variance originally sought.\n",
    "\n",
    "We can force the decision trees to be different by limiting the features (rows) that the greedy algorithm can evaluate at each split point when creating the tree. This is called the Random Forest algorithm.\n",
    "\n",
    "Like bagging, multiple samples of the training dataset are taken and a different tree trained on each. The difference is that at each point a split is made in the data and added to the tree, only a fixed subset of attributes can be considered.\n",
    "\n",
    "For classification problems, the type of problems we will look at in this tutorial, the number of attributes to be considered for the split is limited to the square root of the number of input features.\n",
    "\n",
    "$$\\text{number of features for split} = \\sqrt{\\text{total input features}}$$\n",
    "\n",
    "The result of this one small change are trees that are more different from each other (uncorrelated) resulting in predictions that are more diverse and a combined prediction that often has better performance than single tree or bagging alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
