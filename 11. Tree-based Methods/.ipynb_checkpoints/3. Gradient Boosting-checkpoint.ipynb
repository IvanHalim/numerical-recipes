{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees\n",
    "\n",
    "Written by: [Prince Grover](https://medium.com/@pgrover3)\n",
    "\n",
    "Although most of the Kaggle competition winners use stack/ensemble of various models, one particular model that is part of most of the ensembles is some variant of Gradient Boosting (GBM) algorithm. Take for an example the winner of latest Kaggle competition: [Michael Jahrer](https://www.kaggle.com/mjahrer)'s solution with representation learning in [__Safe Driver Prediction.__](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629#250927) His solution was a blend of 6 models. 1 [LightGBM](https://github.com/Microsoft/LightGBM) (a variant of GBM) and 5 Neural Nets. Although his success is attributed to the semi-supervised learning that he used for the structured data, but gradient boosting model has done the useful part too.\n",
    "\n",
    "Even though GBM is being used widely, many practitioners still treat it as a complex black-box algorithm and just run the models using pre-build libraries. The purpose of this post is to simplify a supposedly complex algorithm and to help the reader to understand the algorithm intuitively. I am going to explain the pure vanilla version of the gradient boosting algorithm and will share links for its different variants at the end. I have taken base DecisionTree code from [__fast.ai__](https://github.com/fastai/fastai) library and on top of that, I have built my own simple version of basic gradient boosting model.\n",
    "\n",
    "## Brief description for Ensemble, Bagging and Boosting\n",
    "\n",
    "When we try to predict the target variable using any machine learning technique, the main causes of difference in actual and predicted values are __noise, variance and bias.__ Ensemble helps to reduce these factors (except noise, which is irreducible error)\n",
    "\n",
    "An ensemble is just a collection of predictors which come together (e.g. mean of all predictions) to give a final prediction. The reason we use ensembles is that many different predictors trying to predict the same target variable will perform a better job than any single predictor alone. Ensembling techniques are further classified into Bagging and Boosting.\n",
    "\n",
    "-  __Bagging__ is a simple ensembling technique in which we build many _independent_ predictors/models/learners and combine them using some model averaging techniques. (e.g. weighted average, majority vote or normal average)\n",
    "\n",
    "We typically take random sub-sample/bootstrap of data for each model, so that all the models are little different from each other. _Each observation is chosen with replacement to be used as input for each of the model. So, each model will have different observations based on the bootstrap process._ Because this technique takes many uncorrelated learners to make a final model, it reduces error by reducing variance. Example of bagging ensemble is __Random Forest models.__\n",
    "\n",
    "-  __Boosting__ is an ensemble technique in which the predictors are not made independently, but sequentially.\n",
    "\n",
    "This technique employs the logic in which the subsequent predictors learn from the mistakes of the previous predictors. _Therefore, the observations have an unequal probability of appearing in subsequent models and ones with the highest error appear most. (So the observations are not chosen based on the bootstrap process, but based on the error)._ The predictors can be chosen from a range of models like decision trees, regressors, classifiers etc. Because new predictors are learning from mistakes committed by previous predictors, it takes less time/iterations to reach close to actual predictions. But we have to choose the stopping criteria carefully or it could lead to overfitting on training data. __Gradient Boosting__ is an example of boosting algorithm.\n",
    "\n",
    "<img src='files/img/ensembling.png'>\n",
    "\n",
    "<img src='files/img/bagboost.png'>\n",
    "\n",
    "## Gradient Boosting algorithm\n",
    "\n",
    "_Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees._ (Wikipedia definition)\n",
    "\n",
    "The objective of any supervised learning algorithm is to define a loss function and minimize it. Let's see how maths work out for Gradient Boosting algorithm. Say we have mean squared error (MSE) as loss defined as:\n",
    "\n",
    "$$Loss = MSE = \\sum(y_i - y_i^p)^2$$\n",
    "\n",
    "where, $y_i = i^{th}$ target value, $y_i^p = i^{th}$ prediction, $L(y_i, y_i^p)$ is Loss function.\n",
    "\n",
    "We want our predictions, such that our loss function (MSE) is minimum. By using __gradient descent__ and updating our predictions based on a learning rate, we can find the values where MSE is minimum.\n",
    "\n",
    "$$y_i^p = y_i^p + \\alpha \\frac{\\partial}{\\partial y_i^p}\\sum(y_i - y_i^p)^2$$\n",
    "\n",
    "which becomes,\n",
    "\n",
    "$$y_i^p = y_i^p - 2 \\alpha \\sum(y_i - y_i^p)$$\n",
    "\n",
    "where, $\\alpha$ is learning rate and $\\sum(y_i - y_i^p)$ is sum of residuals.\n",
    "\n",
    "So, we are basically updating the predictions such that the sum of our residuals is close to 0 (or minimum) and predicted values are sufficiently close to actual values.\n",
    "\n",
    "## Intuition behind Gradient Boosting\n",
    "\n",
    "The logic behind gradient boosting is simple, (can be understood intuitively, without using mathematical notation). I expect that whoever is reading this post might be familiar with `simple linear regression` modeling.\n",
    "\n",
    "A basic assumption of linear regression is that sum of its residuals is 0, i.e. the residuals should be spread randomly around zero.\n",
    "\n",
    "<img src='files/img/residuals.png'>\n",
    "\n",
    "Now think of these residuals as mistakes committed by our predictor model. Although, tree-based models _(considering decision tree as base models for our gradient boosting here)_ are not based on such assumptions, but if we think logically (not statistically) about this assumption, ___we might argue that, if we are able to see some pattern of residuals around 0, we can leverage that pattern to fit a model.___\n",
    "\n",
    "So, the intuition behind `gradient boosting` algorithm is to repetitively leverage the patterns in residuals and strengthen a model with weak predictions and make it better. Once we reach a stage that residuals do not have any pattern that could be modeled, we can stop modeling residuals (otherwise it might lead to overfitting). Algorithmically, we are minimizing our loss function, such that test loss reach its minima.\n",
    "\n",
    "_In summary,_\n",
    "-  _We first model data with simple models and analyze data for errors._\n",
    "-  _These errors signify data points that are difficult to fit by a simple model._\n",
    "-  _Then for later models, we particulary focus on those hard to fit data to get them right._\n",
    "-  _In the end, we combine all the predictors by giving some weights to each predictor._\n",
    "\n",
    "A more technical quotation of the same logic is written in [__Probably Approximately Correct: Nature's Algorithms for Learning and Prospering in a Complex World,__](http://www.amazon.com/dp/0465060722?tag=inspiredalgor-20)\n",
    "\n",
    "_\"The idea is to use the weak learning method several times to get a succession of hypotheses, each one refocused on the examples that the previous ones found difficult and misclassified. ... Note, however, it is not obvious at all how this can be done\"_\n",
    "\n",
    "## Steps to fit a Gradient Boosting model\n",
    "\n",
    "_Let's consider simulated data as shown in scatter plot below with 1 input (x) and 1 output (y) variables._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0, 50)\n",
    "x = pd.DataFrame({'x':x})\n",
    "\n",
    "y1 = np.random.uniform(10, 15, 10)\n",
    "y2 = np.random.uniform(20, 25, 10)\n",
    "y3 = np.random.uniform(0, 5, 10)\n",
    "y4 = np.random.uniform(30, 32, 10)\n",
    "y5 = np.random.uniform(13, 17, 10)\n",
    "\n",
    "y = np.concatenate((y1, y2, y3, y4, y5))\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
